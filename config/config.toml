# Raggiro Configuration

# Processing settings
[processing]
dry_run = false
recursive = true

# Logging settings
[logging]
log_level = "info"
log_to_file = true
log_format = "%(asctime)s - %(levelname)s - %(message)s"
log_date_format = "%Y-%m-%d %H:%M:%S"

# Extraction settings
[extraction]
ocr_enabled = true
ocr_language = "eng"

# Cleaning settings
[cleaning]
remove_headers_footers = true
normalize_whitespace = true
remove_special_chars = true
min_line_length = 3

# Segmentation settings
[segmentation]
use_spacy = true
spacy_model = "en_core_web_sm"
min_section_length = 100
max_chunk_size = 1000
chunk_overlap = 200

# Export settings
[export]
formats = ["markdown", "json"]
include_metadata = true
pretty_json = true

# Vector database settings
[vector_db]
type = "faiss"  # "faiss", "qdrant", "milvus"
faiss_index_type = "L2"  # "L2", "IP", "Cosine"
qdrant_url = "http://localhost:6333"
qdrant_collection = "raggiro"
qdrant_api_key = ""  # Optional API key for Qdrant Cloud
milvus_url = "http://localhost:19530"
milvus_collection = "raggiro"

# LLM settings (shared among components)
[llm]
provider = "ollama"  # "ollama", "llamacpp", "openai", "replicate"
ollama_base_url = "http://localhost:11434"  # Ollama API URL
ollama_timeout = 30  # Timeout in seconds
llamacpp_path = ""  # Path to llama.cpp executable
# OpenAI or Replicate settings (for future compatibility)
api_key = ""
api_url = ""

# Embedding settings
[embedding]
model = "all-MiniLM-L6-v2"  # Model name for embeddings
dimensions = 384  # Embedding dimensions
normalize = true  # Whether to L2-normalize embeddings
device = "cpu"  # "cpu" or "cuda" for GPU acceleration

# Indexing settings
[indexing]
chunk_level = "chunks"  # "chunks", "paragraphs", "sections"
embedding_model = ${embedding.model}  # Inherit from embedding section
vector_db = ${vector_db.type}  # Inherit from vector_db section
qdrant_url = ${vector_db.qdrant_url}  # Inherit from vector_db section
qdrant_collection = ${vector_db.qdrant_collection}  # Inherit from vector_db section
dimensions = ${embedding.dimensions}  # Inherit from embedding section

# Retrieval settings
[retrieval]
embedding_model = ${embedding.model}  # Inherit from embedding section
vector_db = ${vector_db.type}  # Inherit from vector_db section
qdrant_url = ${vector_db.qdrant_url}  # Inherit from vector_db section
qdrant_collection = ${vector_db.qdrant_collection}  # Inherit from vector_db section
top_k = 5
similarity_threshold = 0.6  # Minimum similarity score to include a chunk

# Query rewriting settings
[rewriting]
enabled = true
llm_type = ${llm.provider}  # Inherit from llm section
model_name = "llama3"
temperature = 0.1
max_tokens = 200
ollama_base_url = ${llm.ollama_base_url}  # Inherit from llm section
llamacpp_path = ${llm.llamacpp_path}  # Inherit from llm section
prompt_template = """
You are a helpful assistant that improves user queries for a retrieval system. Your task is to:

1. Understand the original query
2. Make it more specific, detailed, and precise
3. Expand ambiguous terms while preserving the core meaning
4. Add any missing but implied context that would help retrieval
5. Keep the rewritten query concise, focused, and in the form of a question

Original Query: {query}

Rewritten Query: 
"""

# Response generation settings
[generation]
llm_type = ${llm.provider}  # Inherit from llm section
model_name = "mistral"
temperature = 0.7
max_tokens = 1000
ollama_base_url = ${llm.ollama_base_url}  # Inherit from llm section
llamacpp_path = ${llm.llamacpp_path}  # Inherit from llm section
prompt_template = """
You are a helpful assistant that answers questions based on the provided context. Your task is to:

1. Read and understand the user's question
2. Analyze the provided document chunks for relevant information
3. Generate a comprehensive, accurate answer based ONLY on the provided chunks
4. If the chunks don't contain enough information to answer the question, state this clearly
5. Include specific citations in your answer referencing the source documents
6. Format your response clearly with proper paragraphs, bullet points, or numbered lists as appropriate

User Question: {query}

Context Chunks:
{chunks}

Your Answer (include citations to specific documents):
"""

# Pipeline settings
[pipeline]
use_query_rewriting = true
top_k = 5
collect_metrics = true  # Collect metrics during queries

# Testing settings
[testing]
max_concurrency = 1