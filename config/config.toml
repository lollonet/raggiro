# Raggiro Configuration

# Processing settings
[processing]
dry_run = false
recursive = true

# Logging settings
[logging]
log_level = "info"
log_to_file = true
log_format = "%(asctime)s - %(levelname)s - %(message)s"
log_date_format = "%Y-%m-%d %H:%M:%S"

# Extraction settings
[extraction]
ocr_enabled = true
ocr_language = "eng"

# Cleaning settings
[cleaning]
remove_headers_footers = true
normalize_whitespace = true
remove_special_chars = true
min_line_length = 3

# Segmentation settings
[segmentation]
use_spacy = true
spacy_model = "en_core_web_sm"
min_section_length = 100
max_chunk_size = 500  # Reduced from 1000 for more granular chunking
chunk_overlap = 100   # Adjusted for the new chunk size
semantic_chunking = true  # Enable semantic-based chunking
chunking_strategy = "hybrid"  # Options: "size", "semantic", "hybrid"
semantic_similarity_threshold = 0.65  # Threshold for semantic similarity when creating chunks

# Export settings
[export]
formats = ["markdown", "json"]
include_metadata = true
pretty_json = true

# Vector database settings
[vector_db]
type = "faiss"  # "faiss", "qdrant", "milvus"
faiss_index_type = "L2"  # "L2", "IP", "Cosine"
qdrant_url = "http://qdrant:6333"
qdrant_collection = "raggiro"
qdrant_api_key = ""  # Optional API key for Qdrant Cloud
milvus_url = "http://milvus:19530"
milvus_collection = "raggiro"

# LLM settings (shared among components)
[llm]
provider = "ollama"  # "ollama", "llamacpp", "openai"
# Make sure this URL is directly accessible from your execution environment
ollama_base_url = "http://ollama:11434"  # Ollama API URL 
ollama_timeout = 30  # Timeout in seconds
llamacpp_path = ""  # Path to llama.cpp executable

# OpenAI settings
api_key = ""  # API key for OpenAI
api_url = ""  # Optional API URL override (for Azure OpenAI, etc.)
openai_model = "gpt-3.5-turbo"  # Default model for OpenAI

# Embedding settings
[embedding]
model = "all-MiniLM-L6-v2"  # Model name for embeddings
dimensions = 384  # Embedding dimensions
normalize = true  # Whether to L2-normalize embeddings
device = "cpu"  # "cpu" or "cuda" for GPU acceleration

# Indexing settings
[indexing]
chunk_level = "chunks"  # "chunks", "paragraphs", "sections"
embedding_model = ${embedding.model}  # Inherit from embedding section
vector_db = ${vector_db.type}  # Inherit from vector_db section
qdrant_url = ${vector_db.qdrant_url}  # Inherit from vector_db section
qdrant_collection = ${vector_db.qdrant_collection}  # Inherit from vector_db section
dimensions = ${embedding.dimensions}  # Inherit from embedding section

# Retrieval settings
[retrieval]
embedding_model = ${embedding.model}  # Inherit from embedding section
vector_db = ${vector_db.type}  # Inherit from vector_db section
qdrant_url = ${vector_db.qdrant_url}  # Inherit from vector_db section
qdrant_collection = ${vector_db.qdrant_collection}  # Inherit from vector_db section
top_k = 5
similarity_threshold = 0.6  # Minimum similarity score to include a chunk

# Query rewriting settings
[rewriting]
enabled = true
llm_type = ${llm.provider}  # Inherit from llm section
temperature = 0.1
max_tokens = 200

# Model names by provider type
ollama_model = "llama3.2-vision"  # Model name for Ollama
llamacpp_model = "llama3"  # Model name for LLaMA.cpp
openai_model = ${llm.openai_model}  # Inherit from llm section

# Provider-specific settings (inherited from llm section)
ollama_base_url = ${llm.ollama_base_url}
llamacpp_path = ${llm.llamacpp_path}
api_key = ${llm.api_key}  # For OpenAI
api_url = ${llm.api_url}  # For OpenAI
prompt_template = """
You are a helpful assistant that improves user queries for a retrieval system. Your task is to:

1. Understand the original query
2. Make it more specific, detailed, and precise
3. Expand ambiguous terms while preserving the core meaning
4. Add any missing but implied context that would help retrieval
5. Keep the rewritten query concise, focused, and in the form of a question

Original Query: {query}

Rewritten Query: 
"""

# Response generation settings
[generation]
llm_type = ${llm.provider}  # Inherit from llm section
temperature = 0.1
max_tokens = 1000

# Model names by provider type
ollama_model = "llama3.2-vision"  # Model name for Ollama
llamacpp_model = "mistral"  # Model name for LLaMA.cpp
openai_model = ${llm.openai_model}  # Inherit from llm section

# Provider-specific settings (inherited from llm section)
ollama_base_url = ${llm.ollama_base_url}
llamacpp_path = ${llm.llamacpp_path}
api_key = ${llm.api_key}  # For OpenAI
api_url = ${llm.api_url}  # For OpenAI
prompt_template = """
You are a helpful assistant that answers questions based on the provided context. Your task is to:

1. Read and understand the user's question
2. Analyze the provided document chunks for relevant information
3. Generate a comprehensive, accurate answer based ONLY on the provided chunks
4. If the chunks don't contain enough information to answer the question, state this clearly
5. Include specific citations in your answer referencing the source documents
6. Format your response clearly with proper paragraphs, bullet points, or numbered lists as appropriate

User Question: {query}

Context Chunks:
{chunks}

Your Answer (include citations to specific documents):
"""

# Pipeline settings
[pipeline]
use_query_rewriting = true
top_k = 5
collect_metrics = true  # Collect metrics during queries

# Testing settings
[testing]
max_concurrency = 1